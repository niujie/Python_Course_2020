{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用Python从头开始搭建人工神经网络\n",
    "\n",
    "## 层与层\n",
    "\n",
    "首先，大框架如下：\n",
    "\n",
    "1. 通过**输入层**向人工神经网络输入数据。\n",
    "2. 数据在网络中**一层接着一层**“流动”，直到输出。\n",
    "3. 一旦得到了输出，可以计算**误差的标量**。\n",
    "4. 最终可以通过减去误差对参数的**导数**来调整给定的参数（权重或偏差）。\n",
    "5. 步骤4可以迭代进行。\n",
    "\n",
    "最重要的步骤是第**4**步。构建的网络可以任意多层，每层有任意的形式。但如果修改、添加、或者删除网络中的某一层，网络输出就会改变，也就会改变误差，也就会改变误差对参数的导数。所以，需要能够在不考虑网络结构的情况下计算导数，包括不考虑激活函数和损失函数。\n",
    "\n",
    "为了实现以上目的，需要每一层单独处理。\n",
    "\n",
    "## 每一层需要实现什么\n",
    "\n",
    "每一层（全连接层、卷积、最大池化、dropout层等）我们都至少需要创建2样共通的东西，即**输入**和**输出**数据。\n",
    "\n",
    "$$\n",
    "\\mathbf{X}\\rightarrow\\boxed{\\text{layer}}\\rightarrow\\mathbf{Y}\n",
    "$$\n",
    "\n",
    "### 前向传播\n",
    "\n",
    "要点：**上一层的输出是下一层的输入**。\n",
    "\n",
    "$$\n",
    "\\stackrel{\\text{X}}{\\longrightarrow}\\boxed{\\text{Layer 1}}\\stackrel{\\text{H}_1}{\\longrightarrow}\\boxed{\\text{Layer 2}}\\stackrel{\\text{H}_2}{\\longrightarrow}\\boxed{\\text{Layer 3}}\\stackrel{\\text{Y, E}}{\\longrightarrow}\n",
    "$$\n",
    "\n",
    "如上所示，将数据输入第一层，每一层的输出都成为下一层的输入，直到到达网络末端。比较网络输出结果（$Y$）与预期结果（$Y^*$），可以计算误差$\\mathbf{E}$。目标是通过反向传播算法来调整网络中的参数，以**最小化**误差。\n",
    "\n",
    "### 梯度下降\n",
    "\n",
    "通过调整网络中的参数（$\\mathbf{w}$）使得总误差$\\mathbf{E}$**减小**：\n",
    "$$\n",
    "w\\leftarrow w-\\alpha\\frac{\\partial{E}}{\\partial{w}}\n",
    "$$\n",
    "**学习率$\\alpha$**的范围是[0,1]。注意**$\\partial{E}/\\partial{w}$**（$E$对$w$的导数）。**需要能够为网络的任何参数找到该表达式的值，而不管其架构如何。**\n",
    "\n",
    "### 反向传播\n",
    "\n",
    "假设已知一层的**输出误差对输出的导数**（$\\partial{E}/\\partial{Y}$），就应该能够计算**误差对输入的导数**（$\\partial{E}/\\partial{X}$）。\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{X}}\\leftarrow\\boxed{\\text{layer}}\\leftarrow\\frac{\\partial{E}}{\\partial{Y}}\n",
    "$$\n",
    "记住，`E`是**标量**（一个数），而`X`和`Y`是**矩阵**。\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial{E}}{\\partial{X}}&=\\left[\\frac{\\partial{E}}{\\partial{x_1}}\\quad\\frac{\\partial{E}}{\\partial{x_2}}\\quad\\cdots\\quad\\frac{\\partial{E}}{\\partial{x_i}}\\right]\\\\\n",
    "\\frac{\\partial{E}}{\\partial{Y}}&=\\left[\\frac{\\partial{E}}{\\partial{y_1}}\\quad\\frac{\\partial{E}}{\\partial{y_2}}\\quad\\cdots\\quad\\frac{\\partial{E}}{\\partial{y_i}}\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "先暂时不用理会$\\partial{E}/\\partial{X}$。有一个小技巧，可以通过$\\partial{E}/\\partial{Y}$很容易计算得出$\\partial{E}/\\partial{W}$（如果该层有可训练参数），而**无需任何网络结构的信息！**即，使用链式法则：\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{w}}=\\sum_j\\frac{\\partial{E}}{\\partial{y_j}}\\frac{\\partial{y_j}}{\\partial{w}}\n",
    "$$\n",
    "未知数$\\partial{y_j}/\\partial{w}$完全取决于该层如何计算输出。所以如果每层都能计算$\\partial{E}/\\partial{Y}$（$Y$是该层输出），就可以更新该层的参数！\n",
    "\n",
    "### 为什么要算$\\partial{E}/\\partial{X}$?\n",
    "\n",
    "不要忘记，上一层的输出是下一层的输入。这就意味着某一层的$\\partial{E}/\\partial{X}$就是其上一层的$\\partial{E}/\\partial{Y}$！这就是误差反向传播！再次利用链式法则：\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{x_i}}=\\sum_j\\frac{\\partial{E}}{\\partial{y_j}}\\frac{\\partial{y_j}}{\\partial{x_i}}\n",
    "$$\n",
    "上面这个公式是理解反向传播的关键！通过这个公式，就能够从头开始构建整个深度人工神经网络！\n",
    "\n",
    "### 图解反向传播\n",
    "\n",
    "下图中，第3层使用$\\partial{E}/\\partial{Y}$来更新参数，然后将$\\partial{E}/\\partial{H_2}$通过$\\partial{Y}/\\partial{H_2}$传递给上一层。第二层又重复同样操作，直到输入层。\n",
    "\n",
    "![backpropagation](./images/backpropagation.jpg)\n",
    "\n",
    "### 抽象基类：层\n",
    "\n",
    "抽象类*层*，所有其他类型的层都将继承它，用于处理**输入**，**输出**，**前向**和**反向**传播方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self) -> None:\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接层\n",
    "\n",
    "首先定义和实现第一种类型的层：全连接层（Fully Connected, FC）。FC层是最基础的层类型，所有输入神经元与所有输出神经元相连。\n",
    "\n",
    "![fully connected layer](./images/fully_connected_layer.jpg)\n",
    "\n",
    "### 前向传播\n",
    "\n",
    "每个输出神经元的值可根据下式计算：\n",
    "$$\n",
    "y_j=b_j+\\sum_i{x_i w_{ij}}\n",
    "$$\n",
    "可以使用矩阵**点乘**来简单表示：\n",
    "$$\n",
    "X=\\left[x_1\\quad\\cdots\\quad x_i\\right]\\quad\n",
    "W=\\begin{bmatrix}\n",
    "w_{11} & \\cdots & w_{1j}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "w_{i1} & \\cdots & w_{ij}\n",
    "\\end{bmatrix}\\quad\n",
    "B=\\left[b_1\\quad\\cdots\\quad b_j\\right]\n",
    "$$\n",
    "$$\n",
    "Y=XW+B\n",
    "$$\n",
    "\n",
    "### 反向传播\n",
    "假设*可以计算某一层的误差对**该层输出**的导数矩阵（$\\partial{E}/\\partial{Y}$）*。还需要：\n",
    "\n",
    "1. 误差对参数的导数（$\\partial{E}/\\partial{W}$，$\\partial{E}/\\partial{B}$）\n",
    "2. 误差对输入的导数（$\\partial{E}/\\partial{X}$）\n",
    "\n",
    "先计算$\\partial{E}/\\partial{W}$，该矩阵与$W$的维度一样：`i x j`，`i`是输入神经元数量，`j`是输出神经元数量。对每一个权重计算梯度：\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{W}}=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{E}}{\\partial{w_{11}}} & \\cdots & \\frac{\\partial{E}}{\\partial{w_{1j}}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial{E}}{\\partial{w_{i1}}} & \\cdots & \\frac{\\partial{E}}{\\partial{w_{ij}}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "利用链式法则有：\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial{E}}{\\partial{w_{ij}}}&=\\frac{\\partial{E}}{\\partial{y_1}}\\frac{\\partial{y_1}}{\\partial{w_{ij}}}+\\cdots+\\frac{\\partial{E}}{\\partial{y_j}}\\frac{\\partial{y_j}}{\\partial{w_{ij}}}\\\\\n",
    "&=\\frac{\\partial{E}}{\\partial{y_j}}x_i\n",
    "\\end{split}\n",
    "$$\n",
    "因此，\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial{E}}{\\partial{W}}&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{E}}{\\partial{y_1}}x_1 & \\cdots & \\frac{\\partial{E}}{\\partial{y_j}}x_1 \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial{E}}{\\partial{y_1}}x_i & \\cdots & \\frac{\\partial{E}}{\\partial{y_j}}x_i\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_i\n",
    "\\end{bmatrix}\n",
    "\\left[\\frac{\\partial{E}}{\\partial{y_1}}\\quad\\cdots\\quad\\frac{\\partial{E}}{\\partial{y_j}}\\right] \\\\\n",
    "&=X^t\\frac{\\partial{E}}{\\partial{Y}}\n",
    "\\end{split}\n",
    "$$\n",
    "上面是用来更新权重的第一个公式。下面计算$\\frac{\\partial{E}}{\\partial{B}}$。\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{B}}=\\left[\\frac{\\partial{E}}{\\partial{b_1}}\\quad\\frac{\\partial{E}}{\\partial{b_2}}\\quad\\cdots\\quad\\frac{\\partial{E}}{\\partial{b_j}}\\right]\n",
    "$$\n",
    "再次，$\\frac{\\partial{E}}{\\partial{B}}$必须与$B$本身维度相同，一个偏差一个梯度值。再次利用链式法则：\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial{E}}{\\partial{b_j}}&=\\frac{\\partial{E}}{\\partial{y_1}}\\frac{\\partial{y_1}}{\\partial{b_j}}+\\cdots+\\frac{\\partial{E}}{\\partial{y_j}}\\frac{\\partial{y_j}}{\\partial{b_j}}\\\\\n",
    "&=\\frac{\\partial{E}}{\\partial{y_j}}\n",
    "\\end{split}\n",
    "$$\n",
    "也就是，\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial{E}}{\\partial{B}}&=\\left[\\frac{\\partial{E}}{\\partial{y_1}}\\quad\\frac{\\partial{E}}{\\partial{y_2}}\\quad\\cdots\\quad\\frac{\\partial{E}}{\\partial{y_j}}\\right]\\\\\n",
    "&=\\frac{\\partial{E}}{\\partial{Y}}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "现在已经计算了$\\partial{E}/\\partial{W}$和$\\partial{E}/\\partial{B}$，还剩下$\\partial{E}/\\partial{X}$这一个**非常重要**的项，因为它其实“充当”上一层的$\\partial{E}/\\partial{Y}$。\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{X}}=\\left[\\frac{\\partial{E}}{\\partial{x_1}}\\quad\\frac{\\partial{E}}{\\partial{x_2}}\\quad\\cdots\\quad\\frac{\\partial{E}}{\\partial{x_i}}\\right]\n",
    "$$\n",
    "再次利用链式法则：\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial{E}}{\\partial{x_i}}&=\\frac{\\partial{E}}{\\partial{y_1}}\\frac{\\partial{y_1}}{\\partial{x_i}}+\\cdots+\\frac{\\partial{E}}{\\partial{y_j}}\\frac{\\partial{y_j}}{\\partial{x_i}}\\\\\n",
    "&=\\frac{\\partial{E}}{\\partial{y_1}}w_{i1}+\\cdots+\\frac{\\partial{E}}{\\partial{y_j}}w_{ij}\n",
    "\\end{split}\n",
    "$$\n",
    "最终，可以计算整个矩阵：\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial{E}}{\\partial{X}}&=\\left[\\left(\\frac{\\partial{E}}{\\partial{y_1}}w_{11}+\\cdots+\\frac{\\partial{E}}{\\partial{y_j}}w_{1j}\\right)\\quad\\cdots\\quad\\left(\\frac{\\partial{E}}{\\partial{y_1}}w_{i1}+\\cdots+\\frac{\\partial{E}}{\\partial{y_j}}w_{ij}\\right)\\right]\\\\\n",
    "&=\\left[\\frac{\\partial{E}}{\\partial{y_1}}\\quad\\cdots\\quad\\frac{\\partial{E}}{\\partial{y_j}}\\right]\n",
    "\\begin{bmatrix}\n",
    "w_{11} & \\cdots & w_{i1} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "w_{1j} & \\cdots & w_{ij}\n",
    "\\end{bmatrix} \\\\\n",
    "&=\\frac{\\partial{E}}{\\partial{Y}}W^t\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "现在我们有了FC层计算所需的全部三个方程！\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial{E}}{\\partial{X}}&=\\frac{\\partial{E}}{\\partial{Y}}W^t\\\\\n",
    "\\frac{\\partial{E}}{\\partial{W}}&=X^t\\frac{\\partial{E}}{\\partial{Y}}\\\\\n",
    "\\frac{\\partial{E}}{\\partial{B}}&=\\frac{\\partial{E}}{\\partial{Y}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### 全连接层代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size) -> None:\n",
    "        super().__init__()\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "    \n",
    "    # returns output for a iven input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    # computes dE/dW, dE/dB for a given output_error = dE/dY. Returns input_error = dE/dx.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活层\n",
    "到目前为止，所做的所有计算都是完全线性的。用这种模型学任何东西都是没有希望的。需要通过将非线性函数应用于某些层的输出，将**非线性**添加到模型中。\n",
    "\n",
    "现在，我们需要为这种新类型的层重做整个过程！\n",
    "\n",
    "不用担心，由于没有*可学习*的参数，速度会快得多。我们只需要计算一下$\\partial{E}/\\partial{X}$。\n",
    "\n",
    "将激活函数及其导数分别称为`f`和`f'`。\n",
    "\n",
    "#### 前向传播\n",
    "\n",
    "非常直观，对于给定的输入`X`，输出只是简单将激活函数应用于`X`的每个元素。这意味着**输入**和**输出**具有**相同维度**。\n",
    "$$\n",
    "\\begin{split}\n",
    "Y&=\\left[f(x_1)\\quad\\cdots\\quad f(x_i)\\right]\\\\\n",
    "&=f(X)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "#### 反向传播\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial{E}}{\\partial{X}}&=\\left[\\frac{\\partial{E}}{\\partial{x_1}}\\quad\\cdots\\quad\\frac{\\partial{E}}{\\partial{x_i}}\\right]\\\\\n",
    "&=\\left[\\frac{\\partial{E}}{\\partial{y_1}}\\frac{\\partial{y_1}}{\\partial{x_1}}\\quad\\cdots\\quad\\frac{\\partial{E}}{\\partial{y_i}}\\frac{\\partial{y_i}}{\\partial{x_i}}\\right]\\\\\n",
    "&=\\left[\\frac{\\partial{E}}{\\partial{y_1}}f'(x_1)\\quad\\cdots\\quad\\frac{\\partial{E}}{\\partial{y_i}}f'(x_i)\\right]\\\\\n",
    "&=\\left[\\frac{\\partial{E}}{\\partial{y_1}}\\quad\\cdots\\quad\\frac{\\partial{E}}{\\partial{y_i}}\\right]\\cdot\\left[f'(x_1)\\quad\\cdots\\quad f'(x_i)\\right]\\\\\n",
    "&=\\frac{\\partial{E}}{\\partial{Y}}\\cdot f'(X)\n",
    "\\end{split}\n",
    "$$\n",
    "注意，这里我们使用两个矩阵之间的**元素相乘**（而在之前的公式中是点积）。\n",
    "\n",
    "#### 激活函数代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime) -> None:\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "    \n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "    \n",
    "    # Returns input_error = dE/dX for a given output_error = dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单独编写一些激活函数及其导数。这些将在以后用于创建`ActivationLayer`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数\n",
    "\n",
    "到目前为止，我们都假设$\\partial{E}/\\partial{Y}$已知（通过下一层）。但最后一层发生了什么？是怎么计算得到$\\partial{E}/\\partial{Y}$的？取决于如何计算误差。\n",
    "\n",
    "网络错误**人为定义**，它衡量网络对给定输入数据表现得好坏。定义误差的方法有很多种，其中一种最常用的方法叫做**MSE——均方误差**。\n",
    "$$\n",
    "E=\\frac{1}{n}\\sum_i^n\\left(y_i^*-y_i\\right)^2\n",
    "$$\n",
    "其中`y*`和`y`分别代表**期望输出**和**实际输出**。可以把损失函数看作是最后一层，它把所有的输出神经元压缩成一个。我们现在需要对于每一层定义$\\frac{\\partial E}{\\partial Y}$。\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial{E}}{\\partial{Y}}&=\\left[\\frac{\\partial{E}}{\\partial{y_1}}\\quad\\cdots\\quad\\frac{\\partial{E}}{\\partial{y_i}}\\right]\\\\\n",
    "&=\\frac{2}{n}\\left[y_1-y_1^*\\quad\\cdots\\quad y_i-y_i^*\\right]\\\\\n",
    "&=\\frac{2}{n}(Y-Y^*)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self) -> None:\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "    \n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # train the network\n",
    "    def fit(self, x, y, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        samples = len(x)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "                \n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "            \n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d    error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建网络\n",
    "\n",
    "终于我们可以使用我们的类创建一个神经网络，它可以是任意多层！我们将构建两个神经网络：一个简单的**XOR**和一个**MNIST**求解器。\n",
    "\n",
    "#### 解XOR\n",
    "\n",
    "从XOR开始总是很重要的，因为它是判断网络是否在学习任何东西的简单方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000    error=0.367800\n",
      "epoch 2/1000    error=0.325306\n",
      "epoch 3/1000    error=0.311020\n",
      "epoch 4/1000    error=0.304459\n",
      "epoch 5/1000    error=0.300676\n",
      "epoch 6/1000    error=0.298169\n",
      "epoch 7/1000    error=0.296361\n",
      "epoch 8/1000    error=0.294981\n",
      "epoch 9/1000    error=0.293885\n",
      "epoch 10/1000    error=0.292989\n",
      "epoch 11/1000    error=0.292237\n",
      "epoch 12/1000    error=0.291593\n",
      "epoch 13/1000    error=0.291030\n",
      "epoch 14/1000    error=0.290531\n",
      "epoch 15/1000    error=0.290082\n",
      "epoch 16/1000    error=0.289672\n",
      "epoch 17/1000    error=0.289295\n",
      "epoch 18/1000    error=0.288944\n",
      "epoch 19/1000    error=0.288615\n",
      "epoch 20/1000    error=0.288305\n",
      "epoch 21/1000    error=0.288012\n",
      "epoch 22/1000    error=0.287732\n",
      "epoch 23/1000    error=0.287466\n",
      "epoch 24/1000    error=0.287211\n",
      "epoch 25/1000    error=0.286967\n",
      "epoch 26/1000    error=0.286732\n",
      "epoch 27/1000    error=0.286507\n",
      "epoch 28/1000    error=0.286291\n",
      "epoch 29/1000    error=0.286083\n",
      "epoch 30/1000    error=0.285884\n",
      "epoch 31/1000    error=0.285692\n",
      "epoch 32/1000    error=0.285507\n",
      "epoch 33/1000    error=0.285330\n",
      "epoch 34/1000    error=0.285160\n",
      "epoch 35/1000    error=0.284997\n",
      "epoch 36/1000    error=0.284840\n",
      "epoch 37/1000    error=0.284690\n",
      "epoch 38/1000    error=0.284545\n",
      "epoch 39/1000    error=0.284407\n",
      "epoch 40/1000    error=0.284275\n",
      "epoch 41/1000    error=0.284148\n",
      "epoch 42/1000    error=0.284027\n",
      "epoch 43/1000    error=0.283911\n",
      "epoch 44/1000    error=0.283799\n",
      "epoch 45/1000    error=0.283693\n",
      "epoch 46/1000    error=0.283591\n",
      "epoch 47/1000    error=0.283494\n",
      "epoch 48/1000    error=0.283401\n",
      "epoch 49/1000    error=0.283312\n",
      "epoch 50/1000    error=0.283227\n",
      "epoch 51/1000    error=0.283146\n",
      "epoch 52/1000    error=0.283069\n",
      "epoch 53/1000    error=0.282995\n",
      "epoch 54/1000    error=0.282924\n",
      "epoch 55/1000    error=0.282857\n",
      "epoch 56/1000    error=0.282792\n",
      "epoch 57/1000    error=0.282731\n",
      "epoch 58/1000    error=0.282672\n",
      "epoch 59/1000    error=0.282616\n",
      "epoch 60/1000    error=0.282562\n",
      "epoch 61/1000    error=0.282511\n",
      "epoch 62/1000    error=0.282462\n",
      "epoch 63/1000    error=0.282415\n",
      "epoch 64/1000    error=0.282371\n",
      "epoch 65/1000    error=0.282328\n",
      "epoch 66/1000    error=0.282288\n",
      "epoch 67/1000    error=0.282249\n",
      "epoch 68/1000    error=0.282212\n",
      "epoch 69/1000    error=0.282176\n",
      "epoch 70/1000    error=0.282142\n",
      "epoch 71/1000    error=0.282110\n",
      "epoch 72/1000    error=0.282079\n",
      "epoch 73/1000    error=0.282049\n",
      "epoch 74/1000    error=0.282021\n",
      "epoch 75/1000    error=0.281994\n",
      "epoch 76/1000    error=0.281968\n",
      "epoch 77/1000    error=0.281943\n",
      "epoch 78/1000    error=0.281919\n",
      "epoch 79/1000    error=0.281896\n",
      "epoch 80/1000    error=0.281874\n",
      "epoch 81/1000    error=0.281853\n",
      "epoch 82/1000    error=0.281833\n",
      "epoch 83/1000    error=0.281814\n",
      "epoch 84/1000    error=0.281795\n",
      "epoch 85/1000    error=0.281777\n",
      "epoch 86/1000    error=0.281760\n",
      "epoch 87/1000    error=0.281744\n",
      "epoch 88/1000    error=0.281728\n",
      "epoch 89/1000    error=0.281713\n",
      "epoch 90/1000    error=0.281698\n",
      "epoch 91/1000    error=0.281684\n",
      "epoch 92/1000    error=0.281670\n",
      "epoch 93/1000    error=0.281657\n",
      "epoch 94/1000    error=0.281644\n",
      "epoch 95/1000    error=0.281632\n",
      "epoch 96/1000    error=0.281620\n",
      "epoch 97/1000    error=0.281609\n",
      "epoch 98/1000    error=0.281598\n",
      "epoch 99/1000    error=0.281587\n",
      "epoch 100/1000    error=0.281576\n",
      "epoch 101/1000    error=0.281566\n",
      "epoch 102/1000    error=0.281557\n",
      "epoch 103/1000    error=0.281547\n",
      "epoch 104/1000    error=0.281538\n",
      "epoch 105/1000    error=0.281529\n",
      "epoch 106/1000    error=0.281520\n",
      "epoch 107/1000    error=0.281511\n",
      "epoch 108/1000    error=0.281503\n",
      "epoch 109/1000    error=0.281495\n",
      "epoch 110/1000    error=0.281487\n",
      "epoch 111/1000    error=0.281479\n",
      "epoch 112/1000    error=0.281471\n",
      "epoch 113/1000    error=0.281464\n",
      "epoch 114/1000    error=0.281456\n",
      "epoch 115/1000    error=0.281449\n",
      "epoch 116/1000    error=0.281442\n",
      "epoch 117/1000    error=0.281435\n",
      "epoch 118/1000    error=0.281428\n",
      "epoch 119/1000    error=0.281421\n",
      "epoch 120/1000    error=0.281415\n",
      "epoch 121/1000    error=0.281408\n",
      "epoch 122/1000    error=0.281401\n",
      "epoch 123/1000    error=0.281395\n",
      "epoch 124/1000    error=0.281388\n",
      "epoch 125/1000    error=0.281382\n",
      "epoch 126/1000    error=0.281376\n",
      "epoch 127/1000    error=0.281369\n",
      "epoch 128/1000    error=0.281363\n",
      "epoch 129/1000    error=0.281357\n",
      "epoch 130/1000    error=0.281351\n",
      "epoch 131/1000    error=0.281345\n",
      "epoch 132/1000    error=0.281339\n",
      "epoch 133/1000    error=0.281332\n",
      "epoch 134/1000    error=0.281326\n",
      "epoch 135/1000    error=0.281320\n",
      "epoch 136/1000    error=0.281314\n",
      "epoch 137/1000    error=0.281308\n",
      "epoch 138/1000    error=0.281302\n",
      "epoch 139/1000    error=0.281296\n",
      "epoch 140/1000    error=0.281290\n",
      "epoch 141/1000    error=0.281284\n",
      "epoch 142/1000    error=0.281278\n",
      "epoch 143/1000    error=0.281272\n",
      "epoch 144/1000    error=0.281266\n",
      "epoch 145/1000    error=0.281260\n",
      "epoch 146/1000    error=0.281253\n",
      "epoch 147/1000    error=0.281247\n",
      "epoch 148/1000    error=0.281241\n",
      "epoch 149/1000    error=0.281235\n",
      "epoch 150/1000    error=0.281229\n",
      "epoch 151/1000    error=0.281223\n",
      "epoch 152/1000    error=0.281216\n",
      "epoch 153/1000    error=0.281210\n",
      "epoch 154/1000    error=0.281204\n",
      "epoch 155/1000    error=0.281197\n",
      "epoch 156/1000    error=0.281191\n",
      "epoch 157/1000    error=0.281185\n",
      "epoch 158/1000    error=0.281178\n",
      "epoch 159/1000    error=0.281172\n",
      "epoch 160/1000    error=0.281165\n",
      "epoch 161/1000    error=0.281159\n",
      "epoch 162/1000    error=0.281152\n",
      "epoch 163/1000    error=0.281145\n",
      "epoch 164/1000    error=0.281139\n",
      "epoch 165/1000    error=0.281132\n",
      "epoch 166/1000    error=0.281125\n",
      "epoch 167/1000    error=0.281119\n",
      "epoch 168/1000    error=0.281112\n",
      "epoch 169/1000    error=0.281105\n",
      "epoch 170/1000    error=0.281098\n",
      "epoch 171/1000    error=0.281091\n",
      "epoch 172/1000    error=0.281084\n",
      "epoch 173/1000    error=0.281077\n",
      "epoch 174/1000    error=0.281070\n",
      "epoch 175/1000    error=0.281063\n",
      "epoch 176/1000    error=0.281056\n",
      "epoch 177/1000    error=0.281049\n",
      "epoch 178/1000    error=0.281042\n",
      "epoch 179/1000    error=0.281035\n",
      "epoch 180/1000    error=0.281027\n",
      "epoch 181/1000    error=0.281020\n",
      "epoch 182/1000    error=0.281013\n",
      "epoch 183/1000    error=0.281005\n",
      "epoch 184/1000    error=0.280998\n",
      "epoch 185/1000    error=0.280990\n",
      "epoch 186/1000    error=0.280983\n",
      "epoch 187/1000    error=0.280975\n",
      "epoch 188/1000    error=0.280968\n",
      "epoch 189/1000    error=0.280960\n",
      "epoch 190/1000    error=0.280953\n",
      "epoch 191/1000    error=0.280945\n",
      "epoch 192/1000    error=0.280937\n",
      "epoch 193/1000    error=0.280930\n",
      "epoch 194/1000    error=0.280922\n",
      "epoch 195/1000    error=0.280914\n",
      "epoch 196/1000    error=0.280906\n",
      "epoch 197/1000    error=0.280898\n",
      "epoch 198/1000    error=0.280890\n",
      "epoch 199/1000    error=0.280883\n",
      "epoch 200/1000    error=0.280875\n",
      "epoch 201/1000    error=0.280867\n",
      "epoch 202/1000    error=0.280859\n",
      "epoch 203/1000    error=0.280851\n",
      "epoch 204/1000    error=0.280842\n",
      "epoch 205/1000    error=0.280834\n",
      "epoch 206/1000    error=0.280826\n",
      "epoch 207/1000    error=0.280818\n",
      "epoch 208/1000    error=0.280810\n",
      "epoch 209/1000    error=0.280802\n",
      "epoch 210/1000    error=0.280793\n",
      "epoch 211/1000    error=0.280785\n",
      "epoch 212/1000    error=0.280777\n",
      "epoch 213/1000    error=0.280769\n",
      "epoch 214/1000    error=0.280760\n",
      "epoch 215/1000    error=0.280752\n",
      "epoch 216/1000    error=0.280744\n",
      "epoch 217/1000    error=0.280735\n",
      "epoch 218/1000    error=0.280727\n",
      "epoch 219/1000    error=0.280719\n",
      "epoch 220/1000    error=0.280710\n",
      "epoch 221/1000    error=0.280702\n",
      "epoch 222/1000    error=0.280693\n",
      "epoch 223/1000    error=0.280685\n",
      "epoch 224/1000    error=0.280676\n",
      "epoch 225/1000    error=0.280668\n",
      "epoch 226/1000    error=0.280659\n",
      "epoch 227/1000    error=0.280651\n",
      "epoch 228/1000    error=0.280642\n",
      "epoch 229/1000    error=0.280634\n",
      "epoch 230/1000    error=0.280625\n",
      "epoch 231/1000    error=0.280617\n",
      "epoch 232/1000    error=0.280608\n",
      "epoch 233/1000    error=0.280600\n",
      "epoch 234/1000    error=0.280591\n",
      "epoch 235/1000    error=0.280582\n",
      "epoch 236/1000    error=0.280574\n",
      "epoch 237/1000    error=0.280565\n",
      "epoch 238/1000    error=0.280557\n",
      "epoch 239/1000    error=0.280548\n",
      "epoch 240/1000    error=0.280540\n",
      "epoch 241/1000    error=0.280531\n",
      "epoch 242/1000    error=0.280523\n",
      "epoch 243/1000    error=0.280514\n",
      "epoch 244/1000    error=0.280505\n",
      "epoch 245/1000    error=0.280497\n",
      "epoch 246/1000    error=0.280488\n",
      "epoch 247/1000    error=0.280480\n",
      "epoch 248/1000    error=0.280471\n",
      "epoch 249/1000    error=0.280463\n",
      "epoch 250/1000    error=0.280454\n",
      "epoch 251/1000    error=0.280446\n",
      "epoch 252/1000    error=0.280437\n",
      "epoch 253/1000    error=0.280429\n",
      "epoch 254/1000    error=0.280420\n",
      "epoch 255/1000    error=0.280412\n",
      "epoch 256/1000    error=0.280403\n",
      "epoch 257/1000    error=0.280395\n",
      "epoch 258/1000    error=0.280386\n",
      "epoch 259/1000    error=0.280378\n",
      "epoch 260/1000    error=0.280369\n",
      "epoch 261/1000    error=0.280361\n",
      "epoch 262/1000    error=0.280353\n",
      "epoch 263/1000    error=0.280344\n",
      "epoch 264/1000    error=0.280336\n",
      "epoch 265/1000    error=0.280328\n",
      "epoch 266/1000    error=0.280319\n",
      "epoch 267/1000    error=0.280311\n",
      "epoch 268/1000    error=0.280303\n",
      "epoch 269/1000    error=0.280295\n",
      "epoch 270/1000    error=0.280286\n",
      "epoch 271/1000    error=0.280278\n",
      "epoch 272/1000    error=0.280270\n",
      "epoch 273/1000    error=0.280262\n",
      "epoch 274/1000    error=0.280254\n",
      "epoch 275/1000    error=0.280246\n",
      "epoch 276/1000    error=0.280237\n",
      "epoch 277/1000    error=0.280229\n",
      "epoch 278/1000    error=0.280221\n",
      "epoch 279/1000    error=0.280213\n",
      "epoch 280/1000    error=0.280205\n",
      "epoch 281/1000    error=0.280197\n",
      "epoch 282/1000    error=0.280190\n",
      "epoch 283/1000    error=0.280182\n",
      "epoch 284/1000    error=0.280174\n",
      "epoch 285/1000    error=0.280166\n",
      "epoch 286/1000    error=0.280158\n",
      "epoch 287/1000    error=0.280150\n",
      "epoch 288/1000    error=0.280143\n",
      "epoch 289/1000    error=0.280135\n",
      "epoch 290/1000    error=0.280127\n",
      "epoch 291/1000    error=0.280120\n",
      "epoch 292/1000    error=0.280112\n",
      "epoch 293/1000    error=0.280104\n",
      "epoch 294/1000    error=0.280097\n",
      "epoch 295/1000    error=0.280089\n",
      "epoch 296/1000    error=0.280082\n",
      "epoch 297/1000    error=0.280074\n",
      "epoch 298/1000    error=0.280067\n",
      "epoch 299/1000    error=0.280060\n",
      "epoch 300/1000    error=0.280052\n",
      "epoch 301/1000    error=0.280045\n",
      "epoch 302/1000    error=0.280038\n",
      "epoch 303/1000    error=0.280030\n",
      "epoch 304/1000    error=0.280023\n",
      "epoch 305/1000    error=0.280016\n",
      "epoch 306/1000    error=0.280009\n",
      "epoch 307/1000    error=0.280002\n",
      "epoch 308/1000    error=0.279995\n",
      "epoch 309/1000    error=0.279988\n",
      "epoch 310/1000    error=0.279981\n",
      "epoch 311/1000    error=0.279974\n",
      "epoch 312/1000    error=0.279967\n",
      "epoch 313/1000    error=0.279960\n",
      "epoch 314/1000    error=0.279953\n",
      "epoch 315/1000    error=0.279946\n",
      "epoch 316/1000    error=0.279939\n",
      "epoch 317/1000    error=0.279933\n",
      "epoch 318/1000    error=0.279926\n",
      "epoch 319/1000    error=0.279919\n",
      "epoch 320/1000    error=0.279913\n",
      "epoch 321/1000    error=0.279906\n",
      "epoch 322/1000    error=0.279900\n",
      "epoch 323/1000    error=0.279893\n",
      "epoch 324/1000    error=0.279887\n",
      "epoch 325/1000    error=0.279880\n",
      "epoch 326/1000    error=0.279874\n",
      "epoch 327/1000    error=0.279868\n",
      "epoch 328/1000    error=0.279861\n",
      "epoch 329/1000    error=0.279855\n",
      "epoch 330/1000    error=0.279849\n",
      "epoch 331/1000    error=0.279843\n",
      "epoch 332/1000    error=0.279836\n",
      "epoch 333/1000    error=0.279830\n",
      "epoch 334/1000    error=0.279824\n",
      "epoch 335/1000    error=0.279818\n",
      "epoch 336/1000    error=0.279812\n",
      "epoch 337/1000    error=0.279806\n",
      "epoch 338/1000    error=0.279800\n",
      "epoch 339/1000    error=0.279794\n",
      "epoch 340/1000    error=0.279788\n",
      "epoch 341/1000    error=0.279783\n",
      "epoch 342/1000    error=0.279777\n",
      "epoch 343/1000    error=0.279771\n",
      "epoch 344/1000    error=0.279765\n",
      "epoch 345/1000    error=0.279760\n",
      "epoch 346/1000    error=0.279754\n",
      "epoch 347/1000    error=0.279749\n",
      "epoch 348/1000    error=0.279743\n",
      "epoch 349/1000    error=0.279738\n",
      "epoch 350/1000    error=0.279732\n",
      "epoch 351/1000    error=0.279727\n",
      "epoch 352/1000    error=0.279721\n",
      "epoch 353/1000    error=0.279716\n",
      "epoch 354/1000    error=0.279711\n",
      "epoch 355/1000    error=0.279705\n",
      "epoch 356/1000    error=0.279700\n",
      "epoch 357/1000    error=0.279695\n",
      "epoch 358/1000    error=0.279690\n",
      "epoch 359/1000    error=0.279685\n",
      "epoch 360/1000    error=0.279679\n",
      "epoch 361/1000    error=0.279674\n",
      "epoch 362/1000    error=0.279669\n",
      "epoch 363/1000    error=0.279664\n",
      "epoch 364/1000    error=0.279659\n",
      "epoch 365/1000    error=0.279654\n",
      "epoch 366/1000    error=0.279650\n",
      "epoch 367/1000    error=0.279645\n",
      "epoch 368/1000    error=0.279640\n",
      "epoch 369/1000    error=0.279635\n",
      "epoch 370/1000    error=0.279630\n",
      "epoch 371/1000    error=0.279626\n",
      "epoch 372/1000    error=0.279621\n",
      "epoch 373/1000    error=0.279616\n",
      "epoch 374/1000    error=0.279612\n",
      "epoch 375/1000    error=0.279607\n",
      "epoch 376/1000    error=0.279603\n",
      "epoch 377/1000    error=0.279598\n",
      "epoch 378/1000    error=0.279594\n",
      "epoch 379/1000    error=0.279589\n",
      "epoch 380/1000    error=0.279585\n",
      "epoch 381/1000    error=0.279580\n",
      "epoch 382/1000    error=0.279576\n",
      "epoch 383/1000    error=0.279572\n",
      "epoch 384/1000    error=0.279567\n",
      "epoch 385/1000    error=0.279563\n",
      "epoch 386/1000    error=0.279559\n",
      "epoch 387/1000    error=0.279555\n",
      "epoch 388/1000    error=0.279550\n",
      "epoch 389/1000    error=0.279546\n",
      "epoch 390/1000    error=0.279542\n",
      "epoch 391/1000    error=0.279538\n",
      "epoch 392/1000    error=0.279534\n",
      "epoch 393/1000    error=0.279530\n",
      "epoch 394/1000    error=0.279526\n",
      "epoch 395/1000    error=0.279522\n",
      "epoch 396/1000    error=0.279518\n",
      "epoch 397/1000    error=0.279514\n",
      "epoch 398/1000    error=0.279510\n",
      "epoch 399/1000    error=0.279507\n",
      "epoch 400/1000    error=0.279503\n",
      "epoch 401/1000    error=0.279499\n",
      "epoch 402/1000    error=0.279495\n",
      "epoch 403/1000    error=0.279492\n",
      "epoch 404/1000    error=0.279488\n",
      "epoch 405/1000    error=0.279484\n",
      "epoch 406/1000    error=0.279481\n",
      "epoch 407/1000    error=0.279477\n",
      "epoch 408/1000    error=0.279473\n",
      "epoch 409/1000    error=0.279470\n",
      "epoch 410/1000    error=0.279466\n",
      "epoch 411/1000    error=0.279463\n",
      "epoch 412/1000    error=0.279459\n",
      "epoch 413/1000    error=0.279456\n",
      "epoch 414/1000    error=0.279452\n",
      "epoch 415/1000    error=0.279449\n",
      "epoch 416/1000    error=0.279446\n",
      "epoch 417/1000    error=0.279442\n",
      "epoch 418/1000    error=0.279439\n",
      "epoch 419/1000    error=0.279436\n",
      "epoch 420/1000    error=0.279432\n",
      "epoch 421/1000    error=0.279429\n",
      "epoch 422/1000    error=0.279426\n",
      "epoch 423/1000    error=0.279423\n",
      "epoch 424/1000    error=0.279419\n",
      "epoch 425/1000    error=0.279416\n",
      "epoch 426/1000    error=0.279413\n",
      "epoch 427/1000    error=0.279410\n",
      "epoch 428/1000    error=0.279407\n",
      "epoch 429/1000    error=0.279404\n",
      "epoch 430/1000    error=0.279401\n",
      "epoch 431/1000    error=0.279398\n",
      "epoch 432/1000    error=0.279395\n",
      "epoch 433/1000    error=0.279392\n",
      "epoch 434/1000    error=0.279389\n",
      "epoch 435/1000    error=0.279386\n",
      "epoch 436/1000    error=0.279383\n",
      "epoch 437/1000    error=0.279380\n",
      "epoch 438/1000    error=0.279377\n",
      "epoch 439/1000    error=0.279374\n",
      "epoch 440/1000    error=0.279371\n",
      "epoch 441/1000    error=0.279369\n",
      "epoch 442/1000    error=0.279366\n",
      "epoch 443/1000    error=0.279363\n",
      "epoch 444/1000    error=0.279360\n",
      "epoch 445/1000    error=0.279358\n",
      "epoch 446/1000    error=0.279355\n",
      "epoch 447/1000    error=0.279352\n",
      "epoch 448/1000    error=0.279350\n",
      "epoch 449/1000    error=0.279347\n",
      "epoch 450/1000    error=0.279344\n",
      "epoch 451/1000    error=0.279342\n",
      "epoch 452/1000    error=0.279339\n",
      "epoch 453/1000    error=0.279336\n",
      "epoch 454/1000    error=0.279334\n",
      "epoch 455/1000    error=0.279331\n",
      "epoch 456/1000    error=0.279329\n",
      "epoch 457/1000    error=0.279326\n",
      "epoch 458/1000    error=0.279324\n",
      "epoch 459/1000    error=0.279321\n",
      "epoch 460/1000    error=0.279319\n",
      "epoch 461/1000    error=0.279316\n",
      "epoch 462/1000    error=0.279314\n",
      "epoch 463/1000    error=0.279312\n",
      "epoch 464/1000    error=0.279309\n",
      "epoch 465/1000    error=0.279307\n",
      "epoch 466/1000    error=0.279305\n",
      "epoch 467/1000    error=0.279302\n",
      "epoch 468/1000    error=0.279300\n",
      "epoch 469/1000    error=0.279298\n",
      "epoch 470/1000    error=0.279295\n",
      "epoch 471/1000    error=0.279293\n",
      "epoch 472/1000    error=0.279291\n",
      "epoch 473/1000    error=0.279289\n",
      "epoch 474/1000    error=0.279286\n",
      "epoch 475/1000    error=0.279284\n",
      "epoch 476/1000    error=0.279282\n",
      "epoch 477/1000    error=0.279280\n",
      "epoch 478/1000    error=0.279278\n",
      "epoch 479/1000    error=0.279275\n",
      "epoch 480/1000    error=0.279273\n",
      "epoch 481/1000    error=0.279271\n",
      "epoch 482/1000    error=0.279269\n",
      "epoch 483/1000    error=0.279267\n",
      "epoch 484/1000    error=0.279265\n",
      "epoch 485/1000    error=0.279263\n",
      "epoch 486/1000    error=0.279261\n",
      "epoch 487/1000    error=0.279259\n",
      "epoch 488/1000    error=0.279257\n",
      "epoch 489/1000    error=0.279255\n",
      "epoch 490/1000    error=0.279253\n",
      "epoch 491/1000    error=0.279251\n",
      "epoch 492/1000    error=0.279249\n",
      "epoch 493/1000    error=0.279247\n",
      "epoch 494/1000    error=0.279245\n",
      "epoch 495/1000    error=0.279243\n",
      "epoch 496/1000    error=0.279241\n",
      "epoch 497/1000    error=0.279239\n",
      "epoch 498/1000    error=0.279237\n",
      "epoch 499/1000    error=0.279236\n",
      "epoch 500/1000    error=0.279234\n",
      "epoch 501/1000    error=0.279232\n",
      "epoch 502/1000    error=0.279230\n",
      "epoch 503/1000    error=0.279228\n",
      "epoch 504/1000    error=0.279226\n",
      "epoch 505/1000    error=0.279225\n",
      "epoch 506/1000    error=0.279223\n",
      "epoch 507/1000    error=0.279221\n",
      "epoch 508/1000    error=0.279219\n",
      "epoch 509/1000    error=0.279218\n",
      "epoch 510/1000    error=0.279216\n",
      "epoch 511/1000    error=0.279214\n",
      "epoch 512/1000    error=0.279212\n",
      "epoch 513/1000    error=0.279211\n",
      "epoch 514/1000    error=0.279209\n",
      "epoch 515/1000    error=0.279207\n",
      "epoch 516/1000    error=0.279206\n",
      "epoch 517/1000    error=0.279204\n",
      "epoch 518/1000    error=0.279202\n",
      "epoch 519/1000    error=0.279201\n",
      "epoch 520/1000    error=0.279199\n",
      "epoch 521/1000    error=0.279197\n",
      "epoch 522/1000    error=0.279196\n",
      "epoch 523/1000    error=0.279194\n",
      "epoch 524/1000    error=0.279193\n",
      "epoch 525/1000    error=0.279191\n",
      "epoch 526/1000    error=0.279189\n",
      "epoch 527/1000    error=0.279188\n",
      "epoch 528/1000    error=0.279186\n",
      "epoch 529/1000    error=0.279185\n",
      "epoch 530/1000    error=0.279183\n",
      "epoch 531/1000    error=0.279182\n",
      "epoch 532/1000    error=0.279180\n",
      "epoch 533/1000    error=0.279179\n",
      "epoch 534/1000    error=0.279177\n",
      "epoch 535/1000    error=0.279176\n",
      "epoch 536/1000    error=0.279174\n",
      "epoch 537/1000    error=0.279173\n",
      "epoch 538/1000    error=0.279171\n",
      "epoch 539/1000    error=0.279170\n",
      "epoch 540/1000    error=0.279168\n",
      "epoch 541/1000    error=0.279167\n",
      "epoch 542/1000    error=0.279166\n",
      "epoch 543/1000    error=0.279164\n",
      "epoch 544/1000    error=0.279163\n",
      "epoch 545/1000    error=0.279161\n",
      "epoch 546/1000    error=0.279160\n",
      "epoch 547/1000    error=0.279159\n",
      "epoch 548/1000    error=0.279157\n",
      "epoch 549/1000    error=0.279156\n",
      "epoch 550/1000    error=0.279155\n",
      "epoch 551/1000    error=0.279153\n",
      "epoch 552/1000    error=0.279152\n",
      "epoch 553/1000    error=0.279151\n",
      "epoch 554/1000    error=0.279149\n",
      "epoch 555/1000    error=0.279148\n",
      "epoch 556/1000    error=0.279147\n",
      "epoch 557/1000    error=0.279145\n",
      "epoch 558/1000    error=0.279144\n",
      "epoch 559/1000    error=0.279143\n",
      "epoch 560/1000    error=0.279141\n",
      "epoch 561/1000    error=0.279140\n",
      "epoch 562/1000    error=0.279139\n",
      "epoch 563/1000    error=0.279138\n",
      "epoch 564/1000    error=0.279136\n",
      "epoch 565/1000    error=0.279135\n",
      "epoch 566/1000    error=0.279134\n",
      "epoch 567/1000    error=0.279133\n",
      "epoch 568/1000    error=0.279132\n",
      "epoch 569/1000    error=0.279130\n",
      "epoch 570/1000    error=0.279129\n",
      "epoch 571/1000    error=0.279128\n",
      "epoch 572/1000    error=0.279127\n",
      "epoch 573/1000    error=0.279126\n",
      "epoch 574/1000    error=0.279124\n",
      "epoch 575/1000    error=0.279123\n",
      "epoch 576/1000    error=0.279122\n",
      "epoch 577/1000    error=0.279121\n",
      "epoch 578/1000    error=0.279120\n",
      "epoch 579/1000    error=0.279119\n",
      "epoch 580/1000    error=0.279117\n",
      "epoch 581/1000    error=0.279116\n",
      "epoch 582/1000    error=0.279115\n",
      "epoch 583/1000    error=0.279114\n",
      "epoch 584/1000    error=0.279113\n",
      "epoch 585/1000    error=0.279112\n",
      "epoch 586/1000    error=0.279111\n",
      "epoch 587/1000    error=0.279110\n",
      "epoch 588/1000    error=0.279109\n",
      "epoch 589/1000    error=0.279108\n",
      "epoch 590/1000    error=0.279106\n",
      "epoch 591/1000    error=0.279105\n",
      "epoch 592/1000    error=0.279104\n",
      "epoch 593/1000    error=0.279103\n",
      "epoch 594/1000    error=0.279102\n",
      "epoch 595/1000    error=0.279101\n",
      "epoch 596/1000    error=0.279100\n",
      "epoch 597/1000    error=0.279099\n",
      "epoch 598/1000    error=0.279098\n",
      "epoch 599/1000    error=0.279097\n",
      "epoch 600/1000    error=0.279096\n",
      "epoch 601/1000    error=0.279095\n",
      "epoch 602/1000    error=0.279094\n",
      "epoch 603/1000    error=0.279093\n",
      "epoch 604/1000    error=0.279092\n",
      "epoch 605/1000    error=0.279091\n",
      "epoch 606/1000    error=0.279090\n",
      "epoch 607/1000    error=0.279089\n",
      "epoch 608/1000    error=0.279088\n",
      "epoch 609/1000    error=0.279087\n",
      "epoch 610/1000    error=0.279086\n",
      "epoch 611/1000    error=0.279085\n",
      "epoch 612/1000    error=0.279084\n",
      "epoch 613/1000    error=0.279083\n",
      "epoch 614/1000    error=0.279082\n",
      "epoch 615/1000    error=0.279081\n",
      "epoch 616/1000    error=0.279081\n",
      "epoch 617/1000    error=0.279080\n",
      "epoch 618/1000    error=0.279079\n",
      "epoch 619/1000    error=0.279078\n",
      "epoch 620/1000    error=0.279077\n",
      "epoch 621/1000    error=0.279076\n",
      "epoch 622/1000    error=0.279075\n",
      "epoch 623/1000    error=0.279074\n",
      "epoch 624/1000    error=0.279073\n",
      "epoch 625/1000    error=0.279072\n",
      "epoch 626/1000    error=0.279071\n",
      "epoch 627/1000    error=0.279071\n",
      "epoch 628/1000    error=0.279070\n",
      "epoch 629/1000    error=0.279069\n",
      "epoch 630/1000    error=0.279068\n",
      "epoch 631/1000    error=0.279067\n",
      "epoch 632/1000    error=0.279066\n",
      "epoch 633/1000    error=0.279065\n",
      "epoch 634/1000    error=0.279065\n",
      "epoch 635/1000    error=0.279064\n",
      "epoch 636/1000    error=0.279063\n",
      "epoch 637/1000    error=0.279062\n",
      "epoch 638/1000    error=0.279061\n",
      "epoch 639/1000    error=0.279060\n",
      "epoch 640/1000    error=0.279060\n",
      "epoch 641/1000    error=0.279059\n",
      "epoch 642/1000    error=0.279058\n",
      "epoch 643/1000    error=0.279057\n",
      "epoch 644/1000    error=0.279056\n",
      "epoch 645/1000    error=0.279055\n",
      "epoch 646/1000    error=0.279055\n",
      "epoch 647/1000    error=0.279054\n",
      "epoch 648/1000    error=0.279053\n",
      "epoch 649/1000    error=0.279052\n",
      "epoch 650/1000    error=0.279051\n",
      "epoch 651/1000    error=0.279051\n",
      "epoch 652/1000    error=0.279050\n",
      "epoch 653/1000    error=0.279049\n",
      "epoch 654/1000    error=0.279048\n",
      "epoch 655/1000    error=0.279048\n",
      "epoch 656/1000    error=0.279047\n",
      "epoch 657/1000    error=0.279046\n",
      "epoch 658/1000    error=0.279045\n",
      "epoch 659/1000    error=0.279045\n",
      "epoch 660/1000    error=0.279044\n",
      "epoch 661/1000    error=0.279043\n",
      "epoch 662/1000    error=0.279042\n",
      "epoch 663/1000    error=0.279042\n",
      "epoch 664/1000    error=0.279041\n",
      "epoch 665/1000    error=0.279040\n",
      "epoch 666/1000    error=0.279039\n",
      "epoch 667/1000    error=0.279039\n",
      "epoch 668/1000    error=0.279038\n",
      "epoch 669/1000    error=0.279037\n",
      "epoch 670/1000    error=0.279037\n",
      "epoch 671/1000    error=0.279036\n",
      "epoch 672/1000    error=0.279035\n",
      "epoch 673/1000    error=0.279034\n",
      "epoch 674/1000    error=0.279034\n",
      "epoch 675/1000    error=0.279033\n",
      "epoch 676/1000    error=0.279032\n",
      "epoch 677/1000    error=0.279032\n",
      "epoch 678/1000    error=0.279031\n",
      "epoch 679/1000    error=0.279030\n",
      "epoch 680/1000    error=0.279030\n",
      "epoch 681/1000    error=0.279029\n",
      "epoch 682/1000    error=0.279028\n",
      "epoch 683/1000    error=0.279028\n",
      "epoch 684/1000    error=0.279027\n",
      "epoch 685/1000    error=0.279026\n",
      "epoch 686/1000    error=0.279026\n",
      "epoch 687/1000    error=0.279025\n",
      "epoch 688/1000    error=0.279024\n",
      "epoch 689/1000    error=0.279024\n",
      "epoch 690/1000    error=0.279023\n",
      "epoch 691/1000    error=0.279022\n",
      "epoch 692/1000    error=0.279022\n",
      "epoch 693/1000    error=0.279021\n",
      "epoch 694/1000    error=0.279020\n",
      "epoch 695/1000    error=0.279020\n",
      "epoch 696/1000    error=0.279019\n",
      "epoch 697/1000    error=0.279018\n",
      "epoch 698/1000    error=0.279018\n",
      "epoch 699/1000    error=0.279017\n",
      "epoch 700/1000    error=0.279017\n",
      "epoch 701/1000    error=0.279016\n",
      "epoch 702/1000    error=0.279015\n",
      "epoch 703/1000    error=0.279015\n",
      "epoch 704/1000    error=0.279014\n",
      "epoch 705/1000    error=0.279014\n",
      "epoch 706/1000    error=0.279013\n",
      "epoch 707/1000    error=0.279012\n",
      "epoch 708/1000    error=0.279012\n",
      "epoch 709/1000    error=0.279011\n",
      "epoch 710/1000    error=0.279010\n",
      "epoch 711/1000    error=0.279010\n",
      "epoch 712/1000    error=0.279009\n",
      "epoch 713/1000    error=0.279009\n",
      "epoch 714/1000    error=0.279008\n",
      "epoch 715/1000    error=0.279008\n",
      "epoch 716/1000    error=0.279007\n",
      "epoch 717/1000    error=0.279006\n",
      "epoch 718/1000    error=0.279006\n",
      "epoch 719/1000    error=0.279005\n",
      "epoch 720/1000    error=0.279005\n",
      "epoch 721/1000    error=0.279004\n",
      "epoch 722/1000    error=0.279004\n",
      "epoch 723/1000    error=0.279003\n",
      "epoch 724/1000    error=0.279002\n",
      "epoch 725/1000    error=0.279002\n",
      "epoch 726/1000    error=0.279001\n",
      "epoch 727/1000    error=0.279001\n",
      "epoch 728/1000    error=0.279000\n",
      "epoch 729/1000    error=0.279000\n",
      "epoch 730/1000    error=0.278999\n",
      "epoch 731/1000    error=0.278999\n",
      "epoch 732/1000    error=0.278998\n",
      "epoch 733/1000    error=0.278997\n",
      "epoch 734/1000    error=0.278997\n",
      "epoch 735/1000    error=0.278996\n",
      "epoch 736/1000    error=0.278996\n",
      "epoch 737/1000    error=0.278995\n",
      "epoch 738/1000    error=0.278995\n",
      "epoch 739/1000    error=0.278994\n",
      "epoch 740/1000    error=0.278994\n",
      "epoch 741/1000    error=0.278993\n",
      "epoch 742/1000    error=0.278993\n",
      "epoch 743/1000    error=0.278992\n",
      "epoch 744/1000    error=0.278992\n",
      "epoch 745/1000    error=0.278991\n",
      "epoch 746/1000    error=0.278991\n",
      "epoch 747/1000    error=0.278990\n",
      "epoch 748/1000    error=0.278990\n",
      "epoch 749/1000    error=0.278989\n",
      "epoch 750/1000    error=0.278989\n",
      "epoch 751/1000    error=0.278988\n",
      "epoch 752/1000    error=0.278988\n",
      "epoch 753/1000    error=0.278987\n",
      "epoch 754/1000    error=0.278987\n",
      "epoch 755/1000    error=0.278986\n",
      "epoch 756/1000    error=0.278986\n",
      "epoch 757/1000    error=0.278985\n",
      "epoch 758/1000    error=0.278985\n",
      "epoch 759/1000    error=0.278984\n",
      "epoch 760/1000    error=0.278984\n",
      "epoch 761/1000    error=0.278983\n",
      "epoch 762/1000    error=0.278983\n",
      "epoch 763/1000    error=0.278982\n",
      "epoch 764/1000    error=0.278982\n",
      "epoch 765/1000    error=0.278981\n",
      "epoch 766/1000    error=0.278981\n",
      "epoch 767/1000    error=0.278980\n",
      "epoch 768/1000    error=0.278980\n",
      "epoch 769/1000    error=0.278979\n",
      "epoch 770/1000    error=0.278979\n",
      "epoch 771/1000    error=0.278978\n",
      "epoch 772/1000    error=0.278978\n",
      "epoch 773/1000    error=0.278977\n",
      "epoch 774/1000    error=0.278977\n",
      "epoch 775/1000    error=0.278977\n",
      "epoch 776/1000    error=0.278976\n",
      "epoch 777/1000    error=0.278976\n",
      "epoch 778/1000    error=0.278975\n",
      "epoch 779/1000    error=0.278975\n",
      "epoch 780/1000    error=0.278974\n",
      "epoch 781/1000    error=0.278974\n",
      "epoch 782/1000    error=0.278973\n",
      "epoch 783/1000    error=0.278973\n",
      "epoch 784/1000    error=0.278972\n",
      "epoch 785/1000    error=0.278972\n",
      "epoch 786/1000    error=0.278972\n",
      "epoch 787/1000    error=0.278971\n",
      "epoch 788/1000    error=0.278971\n",
      "epoch 789/1000    error=0.278970\n",
      "epoch 790/1000    error=0.278970\n",
      "epoch 791/1000    error=0.278969\n",
      "epoch 792/1000    error=0.278969\n",
      "epoch 793/1000    error=0.278969\n",
      "epoch 794/1000    error=0.278968\n",
      "epoch 795/1000    error=0.278968\n",
      "epoch 796/1000    error=0.278967\n",
      "epoch 797/1000    error=0.278967\n",
      "epoch 798/1000    error=0.278966\n",
      "epoch 799/1000    error=0.278966\n",
      "epoch 800/1000    error=0.278966\n",
      "epoch 801/1000    error=0.278965\n",
      "epoch 802/1000    error=0.278965\n",
      "epoch 803/1000    error=0.278964\n",
      "epoch 804/1000    error=0.278964\n",
      "epoch 805/1000    error=0.278963\n",
      "epoch 806/1000    error=0.278963\n",
      "epoch 807/1000    error=0.278963\n",
      "epoch 808/1000    error=0.278962\n",
      "epoch 809/1000    error=0.278962\n",
      "epoch 810/1000    error=0.278961\n",
      "epoch 811/1000    error=0.278961\n",
      "epoch 812/1000    error=0.278961\n",
      "epoch 813/1000    error=0.278960\n",
      "epoch 814/1000    error=0.278960\n",
      "epoch 815/1000    error=0.278959\n",
      "epoch 816/1000    error=0.278959\n",
      "epoch 817/1000    error=0.278959\n",
      "epoch 818/1000    error=0.278958\n",
      "epoch 819/1000    error=0.278958\n",
      "epoch 820/1000    error=0.278957\n",
      "epoch 821/1000    error=0.278957\n",
      "epoch 822/1000    error=0.278957\n",
      "epoch 823/1000    error=0.278956\n",
      "epoch 824/1000    error=0.278956\n",
      "epoch 825/1000    error=0.278956\n",
      "epoch 826/1000    error=0.278955\n",
      "epoch 827/1000    error=0.278955\n",
      "epoch 828/1000    error=0.278954\n",
      "epoch 829/1000    error=0.278954\n",
      "epoch 830/1000    error=0.278954\n",
      "epoch 831/1000    error=0.278953\n",
      "epoch 832/1000    error=0.278953\n",
      "epoch 833/1000    error=0.278953\n",
      "epoch 834/1000    error=0.278952\n",
      "epoch 835/1000    error=0.278952\n",
      "epoch 836/1000    error=0.278951\n",
      "epoch 837/1000    error=0.278951\n",
      "epoch 838/1000    error=0.278951\n",
      "epoch 839/1000    error=0.278950\n",
      "epoch 840/1000    error=0.278950\n",
      "epoch 841/1000    error=0.278950\n",
      "epoch 842/1000    error=0.278949\n",
      "epoch 843/1000    error=0.278949\n",
      "epoch 844/1000    error=0.278948\n",
      "epoch 845/1000    error=0.278948\n",
      "epoch 846/1000    error=0.278948\n",
      "epoch 847/1000    error=0.278947\n",
      "epoch 848/1000    error=0.278947\n",
      "epoch 849/1000    error=0.278947\n",
      "epoch 850/1000    error=0.278946\n",
      "epoch 851/1000    error=0.278946\n",
      "epoch 852/1000    error=0.278946\n",
      "epoch 853/1000    error=0.278945\n",
      "epoch 854/1000    error=0.278945\n",
      "epoch 855/1000    error=0.278945\n",
      "epoch 856/1000    error=0.278944\n",
      "epoch 857/1000    error=0.278944\n",
      "epoch 858/1000    error=0.278944\n",
      "epoch 859/1000    error=0.278943\n",
      "epoch 860/1000    error=0.278943\n",
      "epoch 861/1000    error=0.278943\n",
      "epoch 862/1000    error=0.278942\n",
      "epoch 863/1000    error=0.278942\n",
      "epoch 864/1000    error=0.278941\n",
      "epoch 865/1000    error=0.278941\n",
      "epoch 866/1000    error=0.278941\n",
      "epoch 867/1000    error=0.278940\n",
      "epoch 868/1000    error=0.278940\n",
      "epoch 869/1000    error=0.278940\n",
      "epoch 870/1000    error=0.278939\n",
      "epoch 871/1000    error=0.278939\n",
      "epoch 872/1000    error=0.278939\n",
      "epoch 873/1000    error=0.278938\n",
      "epoch 874/1000    error=0.278938\n",
      "epoch 875/1000    error=0.278938\n",
      "epoch 876/1000    error=0.278938\n",
      "epoch 877/1000    error=0.278937\n",
      "epoch 878/1000    error=0.278937\n",
      "epoch 879/1000    error=0.278937\n",
      "epoch 880/1000    error=0.278936\n",
      "epoch 881/1000    error=0.278936\n",
      "epoch 882/1000    error=0.278936\n",
      "epoch 883/1000    error=0.278935\n",
      "epoch 884/1000    error=0.278935\n",
      "epoch 885/1000    error=0.278935\n",
      "epoch 886/1000    error=0.278934\n",
      "epoch 887/1000    error=0.278934\n",
      "epoch 888/1000    error=0.278934\n",
      "epoch 889/1000    error=0.278933\n",
      "epoch 890/1000    error=0.278933\n",
      "epoch 891/1000    error=0.278933\n",
      "epoch 892/1000    error=0.278932\n",
      "epoch 893/1000    error=0.278932\n",
      "epoch 894/1000    error=0.278932\n",
      "epoch 895/1000    error=0.278932\n",
      "epoch 896/1000    error=0.278931\n",
      "epoch 897/1000    error=0.278931\n",
      "epoch 898/1000    error=0.278931\n",
      "epoch 899/1000    error=0.278930\n",
      "epoch 900/1000    error=0.278930\n",
      "epoch 901/1000    error=0.278930\n",
      "epoch 902/1000    error=0.278929\n",
      "epoch 903/1000    error=0.278929\n",
      "epoch 904/1000    error=0.278929\n",
      "epoch 905/1000    error=0.278928\n",
      "epoch 906/1000    error=0.278928\n",
      "epoch 907/1000    error=0.278928\n",
      "epoch 908/1000    error=0.278928\n",
      "epoch 909/1000    error=0.278927\n",
      "epoch 910/1000    error=0.278927\n",
      "epoch 911/1000    error=0.278927\n",
      "epoch 912/1000    error=0.278926\n",
      "epoch 913/1000    error=0.278926\n",
      "epoch 914/1000    error=0.278926\n",
      "epoch 915/1000    error=0.278926\n",
      "epoch 916/1000    error=0.278925\n",
      "epoch 917/1000    error=0.278925\n",
      "epoch 918/1000    error=0.278925\n",
      "epoch 919/1000    error=0.278924\n",
      "epoch 920/1000    error=0.278924\n",
      "epoch 921/1000    error=0.278924\n",
      "epoch 922/1000    error=0.278924\n",
      "epoch 923/1000    error=0.278923\n",
      "epoch 924/1000    error=0.278923\n",
      "epoch 925/1000    error=0.278923\n",
      "epoch 926/1000    error=0.278922\n",
      "epoch 927/1000    error=0.278922\n",
      "epoch 928/1000    error=0.278922\n",
      "epoch 929/1000    error=0.278922\n",
      "epoch 930/1000    error=0.278921\n",
      "epoch 931/1000    error=0.278921\n",
      "epoch 932/1000    error=0.278921\n",
      "epoch 933/1000    error=0.278920\n",
      "epoch 934/1000    error=0.278920\n",
      "epoch 935/1000    error=0.278920\n",
      "epoch 936/1000    error=0.278920\n",
      "epoch 937/1000    error=0.278919\n",
      "epoch 938/1000    error=0.278919\n",
      "epoch 939/1000    error=0.278919\n",
      "epoch 940/1000    error=0.278919\n",
      "epoch 941/1000    error=0.278918\n",
      "epoch 942/1000    error=0.278918\n",
      "epoch 943/1000    error=0.278918\n",
      "epoch 944/1000    error=0.278917\n",
      "epoch 945/1000    error=0.278917\n",
      "epoch 946/1000    error=0.278917\n",
      "epoch 947/1000    error=0.278917\n",
      "epoch 948/1000    error=0.278916\n",
      "epoch 949/1000    error=0.278916\n",
      "epoch 950/1000    error=0.278916\n",
      "epoch 951/1000    error=0.278916\n",
      "epoch 952/1000    error=0.278915\n",
      "epoch 953/1000    error=0.278915\n",
      "epoch 954/1000    error=0.278915\n",
      "epoch 955/1000    error=0.278915\n",
      "epoch 956/1000    error=0.278914\n",
      "epoch 957/1000    error=0.278914\n",
      "epoch 958/1000    error=0.278914\n",
      "epoch 959/1000    error=0.278914\n",
      "epoch 960/1000    error=0.278913\n",
      "epoch 961/1000    error=0.278913\n",
      "epoch 962/1000    error=0.278913\n",
      "epoch 963/1000    error=0.278913\n",
      "epoch 964/1000    error=0.278912\n",
      "epoch 965/1000    error=0.278912\n",
      "epoch 966/1000    error=0.278912\n",
      "epoch 967/1000    error=0.278912\n",
      "epoch 968/1000    error=0.278911\n",
      "epoch 969/1000    error=0.278911\n",
      "epoch 970/1000    error=0.278911\n",
      "epoch 971/1000    error=0.278911\n",
      "epoch 972/1000    error=0.278910\n",
      "epoch 973/1000    error=0.278910\n",
      "epoch 974/1000    error=0.278910\n",
      "epoch 975/1000    error=0.278910\n",
      "epoch 976/1000    error=0.278909\n",
      "epoch 977/1000    error=0.278909\n",
      "epoch 978/1000    error=0.278909\n",
      "epoch 979/1000    error=0.278909\n",
      "epoch 980/1000    error=0.278908\n",
      "epoch 981/1000    error=0.278908\n",
      "epoch 982/1000    error=0.278908\n",
      "epoch 983/1000    error=0.278908\n",
      "epoch 984/1000    error=0.278907\n",
      "epoch 985/1000    error=0.278907\n",
      "epoch 986/1000    error=0.278907\n",
      "epoch 987/1000    error=0.278907\n",
      "epoch 988/1000    error=0.278906\n",
      "epoch 989/1000    error=0.278906\n",
      "epoch 990/1000    error=0.278906\n",
      "epoch 991/1000    error=0.278906\n",
      "epoch 992/1000    error=0.278905\n",
      "epoch 993/1000    error=0.278905\n",
      "epoch 994/1000    error=0.278905\n",
      "epoch 995/1000    error=0.278905\n",
      "epoch 996/1000    error=0.278905\n",
      "epoch 997/1000    error=0.278904\n",
      "epoch 998/1000    error=0.278904\n",
      "epoch 999/1000    error=0.278904\n",
      "epoch 1000/1000    error=0.278904\n",
      "[array([[0.52408319]]), array([[0.5275369]]), array([[0.514496]]), array([[0.51801668]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# training data\n",
    "x_train = np.array([[[0, 0]], [[0, 1]], [[1, 0]], [[1, 1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "# network\n",
    "net = Network()\n",
    "net.add(FCLayer(2, 3))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(3, 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# test\n",
    "out = net.predict(x_train)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 求解MNIST\n",
    "\n",
    "我们没有实现卷积层，但这不是问题。我们所需要做的就是重塑我们的数据，使其能够适应一个完全连接的层。\n",
    "\n",
    "*MNIST数据集由0到9的数字图像组成，形状为28x28x1。目标是预测图片上画的数字。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100    error=0.041226\n",
      "epoch 2/100    error=0.020516\n",
      "epoch 3/100    error=0.016475\n",
      "epoch 4/100    error=0.014136\n",
      "epoch 5/100    error=0.012588\n",
      "epoch 6/100    error=0.011391\n",
      "epoch 7/100    error=0.010458\n",
      "epoch 8/100    error=0.009729\n",
      "epoch 9/100    error=0.009053\n",
      "epoch 10/100    error=0.008477\n",
      "epoch 11/100    error=0.007998\n",
      "epoch 12/100    error=0.007552\n",
      "epoch 13/100    error=0.007164\n",
      "epoch 14/100    error=0.006811\n",
      "epoch 15/100    error=0.006539\n",
      "epoch 16/100    error=0.006277\n",
      "epoch 17/100    error=0.006052\n",
      "epoch 18/100    error=0.005826\n",
      "epoch 19/100    error=0.005618\n",
      "epoch 20/100    error=0.005416\n",
      "epoch 21/100    error=0.005245\n",
      "epoch 22/100    error=0.005112\n",
      "epoch 23/100    error=0.005024\n",
      "epoch 24/100    error=0.004912\n",
      "epoch 25/100    error=0.004716\n",
      "epoch 26/100    error=0.004571\n",
      "epoch 27/100    error=0.004517\n",
      "epoch 28/100    error=0.004419\n",
      "epoch 29/100    error=0.004295\n",
      "epoch 30/100    error=0.004237\n",
      "epoch 31/100    error=0.004154\n",
      "epoch 32/100    error=0.004073\n",
      "epoch 33/100    error=0.004003\n",
      "epoch 34/100    error=0.003931\n",
      "epoch 35/100    error=0.003877\n",
      "epoch 36/100    error=0.003782\n",
      "epoch 37/100    error=0.003743\n",
      "epoch 38/100    error=0.003657\n",
      "epoch 39/100    error=0.003646\n",
      "epoch 40/100    error=0.003579\n",
      "epoch 41/100    error=0.003516\n",
      "epoch 42/100    error=0.003427\n",
      "epoch 43/100    error=0.003430\n",
      "epoch 44/100    error=0.003401\n",
      "epoch 45/100    error=0.003324\n",
      "epoch 46/100    error=0.003325\n",
      "epoch 47/100    error=0.003294\n",
      "epoch 48/100    error=0.003253\n",
      "epoch 49/100    error=0.003201\n",
      "epoch 50/100    error=0.003162\n",
      "epoch 51/100    error=0.003144\n",
      "epoch 52/100    error=0.003107\n",
      "epoch 53/100    error=0.003136\n",
      "epoch 54/100    error=0.003112\n",
      "epoch 55/100    error=0.003080\n",
      "epoch 56/100    error=0.003078\n",
      "epoch 57/100    error=0.003045\n",
      "epoch 58/100    error=0.003011\n",
      "epoch 59/100    error=0.002972\n",
      "epoch 60/100    error=0.002952\n",
      "epoch 61/100    error=0.002937\n",
      "epoch 62/100    error=0.002956\n",
      "epoch 63/100    error=0.002907\n",
      "epoch 64/100    error=0.002870\n",
      "epoch 65/100    error=0.002884\n",
      "epoch 66/100    error=0.002843\n",
      "epoch 67/100    error=0.002821\n",
      "epoch 68/100    error=0.002794\n",
      "epoch 69/100    error=0.002811\n",
      "epoch 70/100    error=0.002779\n",
      "epoch 71/100    error=0.002761\n",
      "epoch 72/100    error=0.002729\n",
      "epoch 73/100    error=0.002716\n",
      "epoch 74/100    error=0.002732\n",
      "epoch 75/100    error=0.002726\n",
      "epoch 76/100    error=0.002687\n",
      "epoch 77/100    error=0.002703\n",
      "epoch 78/100    error=0.002690\n",
      "epoch 79/100    error=0.002655\n",
      "epoch 80/100    error=0.002642\n",
      "epoch 81/100    error=0.002635\n",
      "epoch 82/100    error=0.002619\n",
      "epoch 83/100    error=0.002654\n",
      "epoch 84/100    error=0.002595\n",
      "epoch 85/100    error=0.002611\n",
      "epoch 86/100    error=0.002592\n",
      "epoch 87/100    error=0.002566\n",
      "epoch 88/100    error=0.002569\n",
      "epoch 89/100    error=0.002556\n",
      "epoch 90/100    error=0.002552\n",
      "epoch 91/100    error=0.002549\n",
      "epoch 92/100    error=0.002537\n",
      "epoch 93/100    error=0.002519\n",
      "epoch 94/100    error=0.002507\n",
      "epoch 95/100    error=0.002528\n",
      "epoch 96/100    error=0.002500\n",
      "epoch 97/100    error=0.002487\n",
      "epoch 98/100    error=0.002483\n",
      "epoch 99/100    error=0.002479\n",
      "epoch 100/100    error=0.002508\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:], y_train[0:], epochs=100, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(x_test[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values : \n",
      "[array([[-0.00318634,  0.00119289,  0.00265339,  0.00424661,  0.0390102 ,\n",
      "         0.00239168,  0.00136381,  0.9949129 ,  0.00135175, -0.01392251]]), array([[-3.52216509e-03,  1.85962447e-03,  9.94601405e-01,\n",
      "         3.13072808e-03, -2.90730889e-03, -3.88590761e-03,\n",
      "        -2.71682437e-04,  6.15262275e-03,  1.81434855e-03,\n",
      "        -1.00280730e-02]]), array([[ 3.03617481e-03,  9.96978622e-01,  4.25644064e-03,\n",
      "         2.74395676e-03,  1.64727517e-03,  7.31690707e-03,\n",
      "         1.22548390e-04,  2.28465814e-03,  1.57831000e-05,\n",
      "        -1.58416103e-02]])]\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "\n",
      "true values : \n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted values : \")\n",
    "print(out, end='\\n')\n",
    "print(np.abs(np.around(np.array(out), decimals=0)), end=\"\\n\")\n",
    "print(\"\\ntrue values : \")\n",
    "print(y_test[0:3])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}